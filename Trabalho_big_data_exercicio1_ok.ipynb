{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leticiaholsback/Aprendizado/blob/main/Trabalho_big_data_exercicio1_ok.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ockWpfoQ2HIv"
      },
      "source": [
        "# Comandos para realização do trabalho da matéria de Big Data com uso da biblioteca PySpark.\n",
        "\n",
        "## <font color=red>Observação importante:</font>\n",
        "\n",
        "<font color=yellow>Trabalho realizado com uso da biblioteca pandas não será aceito!</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lunxNZmn2HIy"
      },
      "source": [
        "## Upload do arquivo `imdb-reviews-pt-br.csv` para dentro do Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MND3Mjx32HIz",
        "outputId": "c0a5358e-5886-4066-812e-6e830a72aa93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-26 13:53:36--  https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49549692 (47M) [application/zip]\n",
            "Saving to: ‘imdb-reviews-pt-br.zip’\n",
            "\n",
            "imdb-reviews-pt-br. 100%[===================>]  47.25M   282MB/s    in 0.2s    \n",
            "\n",
            "2023-08-26 13:53:38 (282 MB/s) - ‘imdb-reviews-pt-br.zip’ saved [49549692/49549692]\n",
            "\n",
            "Archive:  imdb-reviews-pt-br.zip\n",
            "  inflating: imdb-reviews-pt-br.csv  \n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/N-CPUninter/Big_Data/main/data/imdb-reviews-pt-br.zip -O imdb-reviews-pt-br.zip\n",
        "!unzip imdb-reviews-pt-br.zip\n",
        "!rm imdb-reviews-pt-br.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq3SgoF72HI0"
      },
      "source": [
        "## Instalação manual das dependências para uso do pyspark no Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOyZZFCT2HI1",
        "outputId": "443fa8cf-20c6-4b53-f26f-b7537b6bf178",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=795ff8bafc6dfc28e565141193bb209127123e91d35b2574ab1067da6d577a02\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aruzK3uS2HI1"
      },
      "source": [
        "## Importar, instanciar e criar a SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfKfrFmR2HI1"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "appName = \"PySpark Trabalho de Big Data\"\n",
        "master = \"local\"\n",
        "\n",
        "spark = SparkSession.builder.appName(appName).master(master).getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbKzGdPm2HI2"
      },
      "source": [
        "## Criar spark dataframe do CSV utilizando o método rtead.csv do spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3SthTgK2HI2"
      },
      "outputs": [],
      "source": [
        "imdb_df = spark.read.csv('imdb-reviews-pt-br.csv',\n",
        "                         header=True,\n",
        "                         quote=\"\\\"\",\n",
        "                         escape=\"\\\"\",\n",
        "                         encoding=\"UTF-8\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n"
      ],
      "metadata": {
        "id": "VYf2cKOdKDkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"CalculatingNegativeSentiments\").getOrCreate()"
      ],
      "metadata": {
        "id": "52XY1KlQMpzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_df = df.filter(df.sentiment == \"neg\")"
      ],
      "metadata": {
        "id": "r3QFsmTCMt5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_sum = negative_df.selectExpr(\"sum(id)\").collect()[0][0]\n"
      ],
      "metadata": {
        "id": "PhUEZ6AHMxrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Leticia, RU: 4025500, coonsegui chegar no resultado da soma dos 'id' dos filmes classificados como negativos em:\", total_sum)\n"
      ],
      "metadata": {
        "id": "E-MthZtZM15A",
        "outputId": "30f451dc-7d05-4cd8-8ce1-e5983b05d6d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leticia, RU: 4025500, coonsegui chegar no resultado da soma dos 'id' dos filmes classificados como negativos em: 247015948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def map_sentiment_id(row):\n",
        "    sentiment = row.sentiment\n",
        "    id_value = int(row.id)\n",
        "    return (sentiment, id_value)"
      ],
      "metadata": {
        "id": "GuaZgJX1KExM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"MapSentimentId\").getOrCreate()"
      ],
      "metadata": {
        "id": "PGqYtJ0uKE_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"/content/imdb-reviews-pt-br.csv\", header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "id": "gviv6MYyKFJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_id_rdd = df.rdd.map(map_sentiment_id)\n"
      ],
      "metadata": {
        "id": "XhmrM2SRKFU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentiment_id_rdd.take(10))"
      ],
      "metadata": {
        "id": "QqQzFDS7MAZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Resultado do mapeamento:\")\n",
        "for result in sentiment_id_rdd.take(10):\n",
        "    print(\"Sentiment:\", result[0], \"ID:\", result[1])"
      ],
      "metadata": {
        "id": "OEb0G3mWMAI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentiment_id_rdd.take(10))"
      ],
      "metadata": {
        "id": "9MdEQrIFKQEF",
        "outputId": "34d37ea5-c185-4633-9f00-0e3cdb79e43e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('neg', 1), ('neg', 2), (' exceto Paxton e o sem-teto e todos recebem o que merecem. Os dois únicos negros que podem atuar são o sem-teto e o viciado', 3), ('neg', 4), ('neg', 5), (' excuse me', 6), ('neg', 7), (' forcedly so', 8), (' and more talk. Youll get to see the less than stellar cast of three as they talk on the bus', 9), ('neg', 10)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Resultado do mapeamento:\")\n",
        "for result in sentiment_id_rdd.take(10):\n",
        "    print(\"Sentiment:\", result[0], \"ID:\", result[1])"
      ],
      "metadata": {
        "id": "mwoVEDD6KQOu",
        "outputId": "836fae4c-e665-486e-fa90-8463ce87fb3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado do mapeamento:\n",
            "Sentiment: neg ID: 1\n",
            "Sentiment: neg ID: 2\n",
            "Sentiment:  exceto Paxton e o sem-teto e todos recebem o que merecem. Os dois únicos negros que podem atuar são o sem-teto e o viciado ID: 3\n",
            "Sentiment: neg ID: 4\n",
            "Sentiment: neg ID: 5\n",
            "Sentiment:  excuse me ID: 6\n",
            "Sentiment: neg ID: 7\n",
            "Sentiment:  forcedly so ID: 8\n",
            "Sentiment:  and more talk. Youll get to see the less than stellar cast of three as they talk on the bus ID: 9\n",
            "Sentiment: neg ID: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j9EyyyPkL7C8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEiksQOr2HI3"
      },
      "source": [
        "# Questão 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questão – Qual o valor da soma de todos os campos “id” dos filmes classificados como negativos para o banco de dados “imdb-reviews-pt-br.csv”\n",
        "Leticia 4025500"
      ],
      "metadata": {
        "id": "FA68-lC3McKI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6GIydjP2HI3"
      },
      "source": [
        "## Criar funções de MAP:\n",
        "- Criar função para mapear o \"sentiment\" como chave e o \"id\" como valor do tipo inteiro"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Função MAP - Leticia 4025500"
      ],
      "metadata": {
        "id": "42PwsOoyQcET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n"
      ],
      "metadata": {
        "id": "Ed-yWZaTQaNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()"
      ],
      "metadata": {
        "id": "Y-14cSswVkwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"/content/imdb-reviews-pt-br.csv\", header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "id": "-lkCU6XhQbWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_df = df.filter(df.sentiment == \"neg\")"
      ],
      "metadata": {
        "id": "ILPhvzP3QbfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_function(row):\n",
        "    text = row[\"text_pt\"]\n",
        "    words = text.split()\n",
        "    return [(word, 1) for word in words]"
      ],
      "metadata": {
        "id": "fKTjy9CiVrYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cria funções de REDUCE:\n",
        "- Criar função de reduce para somar os IDs por \"sentiment\"."
      ],
      "metadata": {
        "id": "zCJ432-7RUc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_function(count1, count2):\n",
        "    return count1 + count2"
      ],
      "metadata": {
        "id": "xcARnoAOQ09E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicação do map/reduce e visualização do resultado"
      ],
      "metadata": {
        "id": "NcrRZqOFUhQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = negative_df.rdd.flatMap(map_function).reduceByKey(reduce_function)"
      ],
      "metadata": {
        "id": "sJjjyI_pQ1Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testar contagem\n",
        "result = word_counts.collect()\n",
        "for word, count in result:\n",
        "    print(f\"Palavra: {word}, Contagem: {count}\")"
      ],
      "metadata": {
        "id": "d0n1rGgUV3R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_word_count = word_counts.map(lambda x: x[1]).reduce(reduce_function)"
      ],
      "metadata": {
        "id": "M7YtbHC6Q1Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RU: 4025500 e o total de palavras nos textos negativos:\", total_word_count)\n"
      ],
      "metadata": {
        "id": "bPpl6jEYUsZ5",
        "outputId": "a461a6de-8e20-477e-b934-1efadf35303b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RU: 4025500 e o total de palavras nos textos negativos: 2415179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "d_RrgakTU0yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V2g4APL2HI4"
      },
      "source": [
        "# Questão 2:\n",
        "Prática 02 – Diferença do número de palavras totais de português para inglês dos textos negativos\n",
        "Questão – Contar palavras dos textos negativos e achar diferença de quantidade.\n",
        "ENUNCIADO: Nessa prática você deverá contar todas as palavras existentes nos textos negativos (Português e Inglês) e então deverá encontrar quantas palavras\n",
        "a mais, no total, os textos em português possuem.\n",
        "Para tal, crie um script em Python ou Scala e rode-o com sua máquina virtual Hadoop ou Spark ou PySpark, como feito na prática 1.\n",
        "É necessário se preocupar em filtrar corretamente as avaliações de filmes para que apenas os textos marcados como negativos sejam contabilizados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na-0dgaR2HI4"
      },
      "source": [
        "## Criar funções de MAP:\n",
        "- Criar função para mapear o \"sentiment\" como chave e uma tupla com a soma das palavras de cada texto como valor."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5FyLV1M1ZZAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wmlmDZXKZZIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ7LI-Cc2HI5"
      },
      "source": [
        "## Cria funções de REDUCE:\n",
        "\n",
        "- Criar função de reduce para somar o numero de palavras de cada texto português e inglês por \"sentiment\"."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ML5yY8htaLXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0jMjllBPaLfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A27rYZNd2HI5"
      },
      "source": [
        "## Aplicação do map/reduce e visualização do resultado\n",
        "\n",
        "1. Aplicar o map/reduce no seu dataframe spark e realizar o collect() ao final\n",
        "2. Selecionar os dados referentes aos textos negativos para realizar a subtração.\n",
        "3. Realizar a subtração das contagens de palavras dos textos negativos para obter o resultado final"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a2ugX30YaLqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gnnQB49jaQws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n"
      ],
      "metadata": {
        "id": "kQszKNW577bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"CalculatingNegativeSentiments\").getOrCreate()"
      ],
      "metadata": {
        "id": "vivfxlDv70N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"/content/imdb-reviews-pt-br.csv\", header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "id": "YSjUca06IVYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_df = df.filter(df.sentiment == \"neg\")\n"
      ],
      "metadata": {
        "id": "WEP2RZcxIaDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_sum = negative_df.selectExpr(\"sum(id)\").collect()[0][0]\n"
      ],
      "metadata": {
        "id": "OzOWzSpmIcit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RU: 4025500, consegui chegar a soma dos campos 'id' dos filmes classificados como negativos é:\", total_sum)\n"
      ],
      "metadata": {
        "id": "R2j61fOfIhOG",
        "outputId": "0d56b77e-1125-45d0-83ec-130111f2669c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A soma dos campos 'id' dos filmes classificados como negativos é: 247015948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.textFile(\"/content/imdb-reviews-pt-br.csv\").filter(lambda line: \"id\" not in line)\n"
      ],
      "metadata": {
        "id": "beChCetd82yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc9uYjdl2HI3"
      },
      "outputs": [],
      "source": [
        "def extract_id_and_sentiment(line):\n",
        "    fields = line.split(',')\n",
        "    id = int(fields[0])\n",
        "    sentiment = fields[1]\n",
        "    return (id, sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_rdd = rdd.filter(lambda line: \"neg\" in extract_id_and_sentiment(line)[1])\n"
      ],
      "metadata": {
        "id": "VyWVhAAZ60FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_ids(id1, id2):\n",
        "    return id1 + id2"
      ],
      "metadata": {
        "id": "OMeP8N-b9P5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_sum = filtered_rdd.map(lambda line: extract_id_and_sentiment(line)[1]).reduce(sum_ids)\n"
      ],
      "metadata": {
        "id": "R7R38EoA-dIR",
        "outputId": "ebaee12b-671e-4887-e9ca-78b5f29c7eb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-61f149ec9a1b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mextract_id_and_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1903\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1905\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1906\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 5) (af57b8cd1cea executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\", line 1900, in func\n    initial = next(iterator)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-23-140dfa6c0913>\", line 1, in <lambda>\n  File \"<ipython-input-22-1fb3385db34d>\", line 3, in extract_id_and_sentiment\nValueError: invalid literal for int() with base 10: '\"14\"'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\", line 1900, in func\n    initial = next(iterator)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-23-140dfa6c0913>\", line 1, in <lambda>\n  File \"<ipython-input-22-1fb3385db34d>\", line 3, in extract_id_and_sentiment\nValueError: invalid literal for int() with base 10: '\"14\"'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"A soma dos campos 'id' dos filmes classificados como negativos é:\", total_sum)\n"
      ],
      "metadata": {
        "id": "ILNppCkJ-c5s",
        "outputId": "c1d2a2c3-69ac-46a1-90be-ad6f65f9a5a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-3ba06a5a8028>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A soma dos campos 'id' dos filmes classificados como negativos é:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'total_sum' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcXjsmM62HI3"
      },
      "source": [
        "[texto do link](https://)## Cria funções de REDUCE:\n",
        "\n",
        "- Criar função de reduce para somar os IDs por \"sentiment\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzDH2JPQ2HI4"
      },
      "outputs": [],
      "source": [
        "def reduceByKey(x,y):\n",
        "  # Coloque aqui o seu código para retornar o resultado necessário.\n",
        "  # Apague a linha abaixo para iniciar seu código.\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlLRoRPW2HI4"
      },
      "source": [
        "## Aplicação do map/reduce e visualização do resultado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cM5ucycZ2HI4"
      },
      "outputs": [],
      "source": [
        "# Coloque aqui a sua linha de código para aplicar o map/reduce no seu\n",
        "# dataframe spark e realize o collect() ao final para visualizar os dados.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diferença do número de palavras totais de português para inglês dos textos negativos\n"
      ],
      "metadata": {
        "id": "TzLtHAXdO4PU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrF4bsTN2HI5"
      },
      "outputs": [],
      "source": [
        "def map(x):\n",
        "  # Coloque aqui o seu código para retornar a tupla necessária.\n",
        "  # Apague a linha abaixo para iniciar seu código.\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuRtYN6E2HI5"
      },
      "outputs": [],
      "source": [
        "def reduceByKey(x,y):\n",
        "  # Coloque aqui o seu código para retornar o resultado necessário.\n",
        "  # Apague a linha abaixo para iniciar seu código.\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR8aJFHJ2HI5"
      },
      "outputs": [],
      "source": [
        "# Coloque aqui suas linhas de código final\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}